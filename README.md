# ðŸ’« StarCoder

## What is this about?
ðŸ’« StarCoder is a language model (LM) trained on source code and natural language text. Its training data incorporates more that 80 different programming languages as well as text extracted from github issues and commits and from notebooks. This repository showcases how we can fine-tune this LM on a specific downstream task.

## Installation
To install the dependencies simply run the following command:
```bash
pip install -r requirements.txt
```
Before you run any of the scripts make sure you are logged in and can push to the hub:
```bash
huggingface-cli login
```

## Fine-Tuning
ðŸ’« StarCoder can be fine-tuned to achieve multiple downstream tasks. Our interest here is to fine-tune StarCoder in order to make it follow instructions. [Instruction fine-tuning](https://arxiv.org/pdf/2109.01652.pdf) has gained a lot of attention recently as it proposes a simple framework that teaches language models to align their outputs with human needs. That procedure requires the availability of quality instruction datasets, which contain multiple `instruction - answer` pairs. Unfortunately such datasets are not ubiquitous but thanks to Hugging Face ðŸ¤—'s [datasets](https://github.com/huggingface/datasets) library we can have access to some good proxies. To fine-tune cheaply and efficiently, we use Hugging Face ðŸ¤—'s [PEFT](https://github.com/huggingface/peft) as well as Tim Dettmers' bitsandbytes.

### Code Alpaca 
[Code Alpaca](https://huggingface.co/datasets/HuggingFaceH4/CodeAlpaca_20K) is a dataset of about 20K `prompt - completion` pairs generated by the techniques in the [self-instruct](https://arxiv.org/abs/2212.10560) paper. Each prompt describes a task that is asked by a user and the corresponding completion is the answer to that task as generated by `text-davinci-003`.

To execute the fine-tuning script run the following command:
```bash
python finetune/alpaca.py \
  --model_path="bigcode/large-model"\
  --dataset_name="HuggingFaceH4/CodeAlpaca_20K"\
  --streaming=False\
  --seq_length 2048\
  --max_steps 1000\
  --batch_size 1\
  --gradient_accumulation_steps 16\
  --learning_rate 5e-6\
  --lr_scheduler_type="cosine"\
  --num_warmup_steps 100\
  --weight_decay 0.05\
  --output_dir="./checkpoints"
```
### Stack Exchange
[Stack Exchange](https://en.wikipedia.org/wiki/Stack_Exchange) is a well-known network of Q&A websites on topics in diverse fields. It is a place where a user can ask a question and obtain answers from other users. Those answers are scored and ranked based on their quality. [Stack exchange instruction](https://huggingface.co/datasets/ArmelR/stack-exchange-instruction) is a dataset that was obtained by scrapping the site in order to build a collection of Q&A pairs. A language model can then be fine-tuned on that dataset to make it elicit strong and diverse question-answering skills.

To execute the fine-tuning script run the following command:
```bash
python finetune/SE.py
  --model_path="bigcode/large-model"\
  --dataset_name="ArmelR/stack-exchange-instruction"\
  --subset="data/finetune"\
  --split="train"\
  --size_valid_set 10000\
  --streaming \
  --seq_length 2048\
  --max_steps 1000\
  --batch_size 1\
  --gradient_accumulation_steps 16\
  --learning_rate 1e-4\
  --lr_scheduler_type="cosine"\
  --num_warmup_steps 100\
  --weight_decay 0.05\
  --output_dir="./checkpoints"
```

## Example outputs
